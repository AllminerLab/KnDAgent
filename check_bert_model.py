#!/usr/bin/env python3
"""
BERTæ¨¡å‹æ£€æŸ¥è„šæœ¬
éªŒè¯æœ¬åœ°BERTæ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
"""

import os
import sys
import torch
from transformers import BertTokenizer, BertModel

def check_bert_model():
    """æ£€æŸ¥BERTæ¨¡å‹"""
    print("ğŸ” å¼€å§‹æ£€æŸ¥BERTæ¨¡å‹...")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    bert_path = "./txt_tokenizer"
    tokenizer_path = "./txt_tokenizer"
    
    print(f"BERTæ¨¡å‹è·¯å¾„: {bert_path}")
    print(f"åˆ†è¯å™¨è·¯å¾„: {tokenizer_path}")
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(bert_path):
        print(f"âŒ BERTæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {bert_path}")
        return False
    
    if not os.path.exists(tokenizer_path):
        print(f"âŒ åˆ†è¯å™¨è·¯å¾„ä¸å­˜åœ¨: {tokenizer_path}")
        return False
    
    # æ£€æŸ¥è·¯å¾„å†…å®¹
    print(f"\nğŸ“ æ¨¡å‹ç›®å½•å†…å®¹:")
    try:
        files = os.listdir(bert_path)
        for file in files:
            print(f"  - {file}")
    except Exception as e:
        print(f"  æ— æ³•è¯»å–ç›®å½•: {e}")
        return False
    
    # å°è¯•åŠ è½½åˆ†è¯å™¨
    print(f"\nğŸ”¤ å°è¯•åŠ è½½åˆ†è¯å™¨...")
    try:
        tokenizer = BertTokenizer.from_pretrained(tokenizer_path)
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•åˆ†è¯
        test_text = "åŒ»ç–—çŸ¥è¯†å›¾è°±"
        tokens = tokenizer.tokenize(test_text)
        print(f"  æµ‹è¯•åˆ†è¯: '{test_text}' -> {tokens}")
        
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return False
    
    # å°è¯•åŠ è½½æ¨¡å‹
    print(f"\nğŸ¤– å°è¯•åŠ è½½BERTæ¨¡å‹...")
    try:
        model = BertModel.from_pretrained(bert_path)
        print("âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
        print(f"  æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æ£€æŸ¥è®¾å¤‡
        device = next(model.parameters()).device
        print(f"  æ¨¡å‹è®¾å¤‡: {device}")
        
        # æµ‹è¯•æ¨ç†
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†...")
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True)
        
        with torch.no_grad():
            outputs = model(**inputs)
            embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token
            print(f"  è¾“å‡ºå½¢çŠ¶: {embeddings.shape}")
            print(f"  è¾“å‡ºç±»å‹: {embeddings.dtype}")
        
        print("âœ… æ¨¡å‹æ¨ç†æµ‹è¯•æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    print(f"\nğŸ‰ BERTæ¨¡å‹æ£€æŸ¥å®Œæˆï¼")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ BERTæ¨¡å‹æ£€æŸ¥å·¥å…·")
    print("=" * 50)
    
    try:
        success = check_bert_model()
        
        if success:
            print("\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼BERTæ¨¡å‹å¯ä»¥æ­£å¸¸ä½¿ç”¨")
            print("\nç°åœ¨å¯ä»¥å¯åŠ¨åŒ»ç–—çŸ¥è¯†å›¾è°±æ¨èAgent:")
            print("python start_medical_agent.py")
        else:
            print("\nâŒ æ£€æŸ¥å¤±è´¥ï¼è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
            
    except Exception as e:
        print(f"\nğŸ’¥ æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
